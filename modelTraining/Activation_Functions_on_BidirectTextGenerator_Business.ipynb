{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Activation Functions on BidirectTextGenerator Business",
      "provenance": [],
      "collapsed_sections": [
        "Bxr1dtLGspGe",
        "se1ab9b-AuHi",
        "qCKqv9pYQAgc",
        "o4IUMwLfQAg9",
        "UCuCYspPNabL",
        "cQYNiOwdNabg"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMGBHcHdDeSrarZH4biMtbU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmaciver/newsflash/blob/master/modelTraining/Activation_Functions_on_BidirectTextGenerator_Business.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e7ecdb84-33a4-42af-e8b1-7b0757486164",
        "id": "58jDtWrqrgH5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVAOC2ppspEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import os\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4PbDwFpspFx",
        "colab_type": "text"
      },
      "source": [
        "## Create Train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzq4lxjyJSmc",
        "colab_type": "code",
        "outputId": "4b99a1e3-a6b4-43e0-a470-4e28b1e77099",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Read Vocabulary of data\n",
        "vocab = []\n",
        "\n",
        "with open('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/LSTM_model/DocsToLoad/vocab.data', 'rb') as filehandle:\n",
        "    # read the data as binary data stream\n",
        "    vocab = pickle.load(filehandle)\n",
        "    \n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5567 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_QQT2MbKURc",
        "colab_type": "code",
        "outputId": "72ddcffb-36f5-4221-880f-18a4e521663c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Read embedded spaces of vocab data\n",
        "\n",
        "embedded = np.load('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/LSTM_model/DocsToLoad/embedded.npy')\n",
        "\n",
        "print(embedded.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5567, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TTWXzjSspFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Separate text file by lines\n",
        "TextLines = open('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/Dataset/data.txt').readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-ln331MspF1",
        "colab_type": "code",
        "outputId": "e6863fd7-3c2b-4aaa-9e44-0743bcc7ded6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len(TextLines)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31898"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqdzzDfxspF3",
        "colab_type": "code",
        "outputId": "490d424c-e967-4b2d-a891-c26f6cdf6e15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Validation list\n",
        "TrainLines, TestLines = train_test_split(TextLines, test_size=0.10, random_state=42)\n",
        "\n",
        "print(len(TrainLines),len(TestLines))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28708 3190\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f4uV0QhspF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text_sequences(Lines, pastWords, vocab):\n",
        "    X_line = list()\n",
        "    Y_line = list()\n",
        "    pastWords = pastWords\n",
        "    for line in Lines:\n",
        "        # Tokenize line\n",
        "        lineTokenized = text_to_word_sequence(line,  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r0123456789'+\"'\")\n",
        "        #Get line length\n",
        "        lengthLine = len(lineTokenized)\n",
        "        lineBatch = lengthLine-pastWords\n",
        "        \n",
        "        # Substitute words outside vocab with <Unkown>\n",
        "        for idx in range(0,len(lineTokenized)):\n",
        "            if lineTokenized[idx] in vocab:\n",
        "                continue\n",
        "            else:\n",
        "                lineTokenized[idx] = '<Unkown>'\n",
        "        \n",
        "        #Crate sequences of text \n",
        "        for i in range(0,lineBatch):\n",
        "            X_sequence = lineTokenized[i:i+pastWords]\n",
        "            X_line.append(X_sequence)\n",
        "            Y_sequence = lineTokenized[i+pastWords]\n",
        "            Y_line.append(Y_sequence)\n",
        "    \n",
        "    return(X_line, Y_line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "143jV1muspF_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pastWords = 5  # number of words to look back for prediction\n",
        "X_lineTrain, Y_lineTrain = generate_text_sequences(TrainLines, pastWords, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhC2aoUlspGE",
        "colab_type": "code",
        "outputId": "c0ecc050-59b9-4b1c-ea16-450c46e66a84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(len(X_lineTrain), len(Y_lineTrain))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "923443 923443\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY6cwk43spGH",
        "colab_type": "text"
      },
      "source": [
        "Creating a batch generator for training data. Converting the whole dataset will take too much memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnsD7DaFspGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "import random as rand\n",
        "from random import randint\n",
        "from random import seed\n",
        "rand.seed(4)\n",
        "\n",
        "def batch_generator_data(batchsize, X_line, Y_line, embDim, pastWords, embedded, vocab):\n",
        "    embDim=embDim\n",
        "    pastWords = pastWords\n",
        "    x_batch = np.zeros(shape=(batchsize,pastWords,embDim))\n",
        "    y_batch = np.zeros(shape=(batchsize))\n",
        "\n",
        "    while True:\n",
        "        # Fill the batch with random continuous sequences of data.\n",
        "\n",
        "        # Get a random start-index.\n",
        "        # This points somewhere into the data.\n",
        "        idx = np.random.randint(len(X_line) - batchsize)\n",
        "\n",
        "        for i in range(0,batchsize):\n",
        "            x_batch[i] = [embedded[vocab.index(x)] for x in X_line[idx+i]]\n",
        "            y_batch[i] = vocab.index(Y_line[idx+i])\n",
        "\n",
        "        #y_batch = to_categorical(y_batch, num_classes=len(vocab))\n",
        "        \n",
        "        yield (x_batch, y_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RPzoBNpspGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embDim = 100 #shape of the embbeded latent space\n",
        "batchsize = 300 #batch size for each training step\n",
        "generator = batch_generator_data(batchsize,X_lineTrain, Y_lineTrain, embDim, pastWords, embedded, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1r3F-aFspGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_batch, Y_train_batch = next(generator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieqsti0BspGR",
        "colab_type": "code",
        "outputId": "ef8a446a-f448-496d-ebff-271a40eb6502",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(X_train_batch.shape)\n",
        "print(Y_train_batch.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(300, 5, 100)\n",
            "(300,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN20nYf7spGU",
        "colab_type": "text"
      },
      "source": [
        "**Generate Validation data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grZtUqW3spGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_lineTest, Y_lineTest = generate_text_sequences(TestLines, pastWords, vocab)\n",
        "valgenerator = batch_generator_data(batchsize,X_lineTest, Y_lineTest, embDim, pastWords, embedded, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2aFt930H_Kt",
        "colab_type": "code",
        "outputId": "2d62ff31-f5cd-4d44-dda7-683208ef4feb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(len(X_lineTest), len(Y_lineTest))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "103138 103138\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyoxdaynspGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test_batch, Y_test_batch = next(valgenerator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0uQYPu_spGa",
        "colab_type": "code",
        "outputId": "181732ba-eb00-47c7-b44c-07b8b3ad726a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(X_test_batch.shape)\n",
        "print(Y_test_batch.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(300, 5, 100)\n",
            "(300,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxr1dtLGspGe",
        "colab_type": "text"
      },
      "source": [
        "## Create Model  Relu\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-SoSdV7spGe",
        "colab_type": "code",
        "outputId": "1ff2f8fc-acab-43d0-ca1a-085c9b3bbba1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, GRU, LSTM, Bidirectional\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from keras import losses\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=500, return_sequences=True,input_shape=(pastWords,embDim), activation='relu'))\n",
        "model.add(LSTM(units=200))\n",
        "model.add(Dense(len(vocab), activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_2 (LSTM)                (None, 5, 500)            1202000   \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 200)               560800    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5567)              1118967   \n",
            "=================================================================\n",
            "Total params: 2,881,767\n",
            "Trainable params: 2,881,767\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Skz2e-zTspGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "optimizer = RMSprop(lr=0.001)\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "model_file = \"/content/drive/My Drive/SharpestMinds/3. Generate Language Model/ActivationFun_BidirectionalModel/Relu/Business(LSTM).{epoch:02d}-{loss:.4f}.h5\"\n",
        "\n",
        "mc = ModelCheckpoint(model_file, monitor=\"loss\", mode=\"min\", save_best_only=True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=4, min_lr=1e-4)\n",
        "\n",
        "es = EarlyStopping(monitor='loss', min_delta=0.01, patience=5, mode='min')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjOcGu9zspGl",
        "colab_type": "code",
        "outputId": "1f1cdd6d-4175-48d9-ce38-f9cd522268b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 787
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "history = model.fit_generator(generator=generator,\n",
        "                    epochs=20,\n",
        "                    steps_per_epoch= len(X_lineTrain)//batchsize,\n",
        "                    validation_data=valgenerator,\n",
        "                    validation_steps= len(X_lineTest)//batchsize,\n",
        "                    callbacks=[mc, reduce_lr, es])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <timed exec>:7: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/20\n",
            "3078/3078 [==============================] - 1377s 448ms/step - loss: 5.6802 - val_loss: 5.2147 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "3078/3078 [==============================] - 1373s 446ms/step - loss: 4.9405 - val_loss: 4.9641 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "3078/3078 [==============================] - 1381s 449ms/step - loss: 4.7128 - val_loss: 4.8873 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "3078/3078 [==============================] - 1367s 444ms/step - loss: 4.5724 - val_loss: 4.9287 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "3078/3078 [==============================] - 1373s 446ms/step - loss: 4.4937 - val_loss: 4.9286 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "3078/3078 [==============================] - 1369s 445ms/step - loss: 4.4360 - val_loss: 4.9282 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "3078/3078 [==============================] - 1375s 447ms/step - loss: 4.3816 - val_loss: 4.9594 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "3078/3078 [==============================] - 1374s 446ms/step - loss: 4.2488 - val_loss: 4.9335 - lr: 2.0000e-04\n",
            "Epoch 9/20\n",
            "3078/3078 [==============================] - 1383s 449ms/step - loss: 4.1745 - val_loss: 4.9491 - lr: 2.0000e-04\n",
            "Epoch 10/20\n",
            "3078/3078 [==============================] - 1383s 449ms/step - loss: 4.1452 - val_loss: 4.9535 - lr: 2.0000e-04\n",
            "Epoch 11/20\n",
            "3078/3078 [==============================] - 1378s 448ms/step - loss: 4.1247 - val_loss: 4.9366 - lr: 2.0000e-04\n",
            "Epoch 12/20\n",
            "3078/3078 [==============================] - 1382s 449ms/step - loss: 4.0751 - val_loss: 4.9569 - lr: 1.0000e-04\n",
            "Epoch 13/20\n",
            "3078/3078 [==============================] - 1403s 456ms/step - loss: 4.0511 - val_loss: 4.9494 - lr: 1.0000e-04\n",
            "Epoch 14/20\n",
            "3078/3078 [==============================] - 1382s 449ms/step - loss: 4.0201 - val_loss: 4.9625 - lr: 1.0000e-04\n",
            "Epoch 15/20\n",
            "3078/3078 [==============================] - 1381s 449ms/step - loss: 4.0045 - val_loss: 5.0026 - lr: 1.0000e-04\n",
            "Epoch 16/20\n",
            "3078/3078 [==============================] - 1387s 450ms/step - loss: 3.9760 - val_loss: 5.0166 - lr: 1.0000e-04\n",
            "Epoch 17/20\n",
            "3078/3078 [==============================] - 1389s 451ms/step - loss: 3.9772 - val_loss: 4.9952 - lr: 1.0000e-04\n",
            "Epoch 18/20\n",
            "3078/3078 [==============================] - 1386s 450ms/step - loss: 3.9490 - val_loss: 5.0211 - lr: 1.0000e-04\n",
            "Epoch 19/20\n",
            "3078/3078 [==============================] - 1375s 447ms/step - loss: 3.9459 - val_loss: 5.0291 - lr: 1.0000e-04\n",
            "Epoch 20/20\n",
            "3078/3078 [==============================] - 1382s 449ms/step - loss: 3.9256 - val_loss: 5.0505 - lr: 1.0000e-04\n",
            "CPU times: user 14h 21min 11s, sys: 18min 22s, total: 14h 39min 34s\n",
            "Wall time: 7h 40min 10s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo2ZC5dgvrzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se1ab9b-AuHi",
        "colab_type": "text"
      },
      "source": [
        "## Load Model, Vocab and Embedded if already trained "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFXBmO1rspGn",
        "colab_type": "code",
        "outputId": "355f2d31-9777-4c92-df61-6398d30f545f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "model = tf.keras.models.load_model('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/ActivationFun_BidirectionalModel/Relu/Business(LSTM).20-3.9256.h5')\n",
        "\n",
        "# Read Vocabulary of data\n",
        "vocab = []\n",
        "\n",
        "with open('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/LSTM_model/DocsToLoad/vocab.data', 'rb') as filehandle:\n",
        "    # read the data as binary data stream\n",
        "    vocab = pickle.load(filehandle)\n",
        "    \n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5567 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFGALlHuX61B",
        "colab_type": "code",
        "outputId": "6fae0dd2-5472-4e7f-f4cb-6ebd7fb5d2c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Read embedded spaces of vocab data\n",
        "\n",
        "embedded = np.load('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/LSTM_model/DocsToLoad/embedded.npy')\n",
        "\n",
        "print(embedded.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5567, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjNxjH4YYDXS",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate Model Relu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO90HtysYEzj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generateInputArray(inputs):\n",
        "  embDim = embedded.shape[1]\n",
        "  x_sample =[]\n",
        "  for x in inputs:\n",
        "    if x in vocab:\n",
        "      x_sample.append(embedded[list(vocab).index(x)])\n",
        "    else:\n",
        "      x_sample.append(np.zeros(embDim))\n",
        "\n",
        "  x_sample = np.array(x_sample)\n",
        "  x_sample = np.expand_dims(x_sample, axis=0)\n",
        "  return(x_sample)\n",
        "  \n",
        "def generatecandidates(text):\n",
        "  textToken = tf.keras.preprocessing.text.text_to_word_sequence(str(text),  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r0123456789'+\"'\")   \n",
        "  \n",
        "  x_sample = generateInputArray(textToken[-5:])\n",
        "  \n",
        "  y_sample = model.predict(x_sample)\n",
        "  \n",
        "  # Get top 50 candidates\n",
        "  ind = np.argpartition(y_sample[0,:], -50)[-50:]\n",
        "\n",
        "  candidates=dict()\n",
        "  for i in ind:\n",
        "    if vocab[i] != \"<Unkown>\":\n",
        "      candidates[vocab[i]] = y_sample[0,i]\n",
        "\n",
        "  return(candidates)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGXo9QKmYQ3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_line, Y_line = generate_text_sequences(TestLines, pastWords, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-bHqeCEYSzp",
        "colab_type": "code",
        "outputId": "afabf658-dbf0-4fca-d120-c42d85fef6fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "points=0\n",
        "TotalPredictions = 0\n",
        "for i in range(0, len(TestLines)):\n",
        "    # Verify if the correct word is in the top-50 words predicted\n",
        "    candidates = generatecandidates(X_line[i])\n",
        "    if Y_line[i] == '<Unkown>':\n",
        "        continue\n",
        "    else:\n",
        "        if Y_line[i] in candidates.keys():\n",
        "            word_prob = list(candidates.keys()).index(Y_line[i])\n",
        "            p = np.array(list(candidates.values()))\n",
        "            points += 1 - p[word_prob] \n",
        "            TotalPredictions +=1\n",
        "        else:\n",
        "            points+=1\n",
        "            TotalPredictions +=1\n",
        "\n",
        "print(\"Average uncertanty of the model \", (points/TotalPredictions)*100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average uncertanty of the model  82.78999014356532 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO2HWLtEjfuT",
        "colab_type": "code",
        "outputId": "a97f820c-015b-4d00-b06f-aad76bab5c42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "points=0\n",
        "TotalPredictions = 0\n",
        "\n",
        "for i in range(0, len(TestLines)):\n",
        "    # Verify if the correct word is in the words predicted\n",
        "    candidates = generatecandidates(X_line[i])\n",
        "    if Y_line[i] == '<Unkown>':\n",
        "        continue\n",
        "    else:\n",
        "        if Y_line[i] in candidates.keys():\n",
        "            points+=1\n",
        "            TotalPredictions +=1\n",
        "        else:\n",
        "            TotalPredictions +=1\n",
        "    \n",
        "    \n",
        "print(\"Percentage of times next word was on top 50 predictions: \",(points/TotalPredictions)*100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of times next word was on top 50 predictions:  63.54063836788417 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qCKqv9pYQAgc"
      },
      "source": [
        "## Create Model  Elu\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4c6e13a2-4338-43c8-fa0d-97d2c9c20680",
        "id": "TRLW0lULQAge",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, GRU, LSTM, Bidirectional\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from keras import losses\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=500, return_sequences=True,input_shape=(pastWords,embDim), activation='elu'))\n",
        "model.add(LSTM(units=200))\n",
        "model.add(Dense(len(vocab), activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 5, 500)            1202000   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 200)               560800    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5567)              1118967   \n",
            "=================================================================\n",
            "Total params: 2,881,767\n",
            "Trainable params: 2,881,767\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1oMw4ksSQAgo",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "optimizer = RMSprop(lr=0.001)\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "model_file = \"/content/drive/My Drive/SharpestMinds/3. Generate Language Model/ActivationFun_BidirectionalModel/Elu/Business(LSTM).{epoch:02d}-{loss:.4f}.h5\"\n",
        "\n",
        "mc = ModelCheckpoint(model_file, monitor=\"loss\", mode=\"min\", save_best_only=True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=4, min_lr=1e-4)\n",
        "\n",
        "es = EarlyStopping(monitor='loss', min_delta=0.01, patience=5, mode='min')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "cd64a56b-ca5d-4f77-d69f-63a4748081da",
        "id": "cq8epQA-QAgw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 787
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "history = model.fit_generator(generator=generator,\n",
        "                    epochs=20,\n",
        "                    steps_per_epoch= len(X_lineTrain)//batchsize,\n",
        "                    validation_data=valgenerator,\n",
        "                    validation_steps= len(X_lineTest)//batchsize,\n",
        "                    callbacks=[mc, reduce_lr, es])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <timed exec>:7: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/20\n",
            "3078/3078 [==============================] - 1367s 444ms/step - loss: 5.6795 - val_loss: 5.1833 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "3078/3078 [==============================] - 1394s 453ms/step - loss: 4.9405 - val_loss: 4.9705 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "3078/3078 [==============================] - 1428s 464ms/step - loss: 4.6903 - val_loss: 4.9067 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "3078/3078 [==============================] - 1388s 451ms/step - loss: 4.5334 - val_loss: 4.8817 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "3078/3078 [==============================] - 1422s 462ms/step - loss: 4.4433 - val_loss: 4.9146 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "3078/3078 [==============================] - 1416s 460ms/step - loss: 4.3644 - val_loss: 4.9343 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "3078/3078 [==============================] - 1432s 465ms/step - loss: 4.3292 - val_loss: 4.9875 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "3078/3078 [==============================] - 1393s 453ms/step - loss: 4.2621 - val_loss: 5.0105 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "3078/3078 [==============================] - 1418s 461ms/step - loss: 4.1074 - val_loss: 5.0326 - lr: 2.0000e-04\n",
            "Epoch 10/20\n",
            "3078/3078 [==============================] - 1377s 447ms/step - loss: 4.0118 - val_loss: 5.0314 - lr: 2.0000e-04\n",
            "Epoch 11/20\n",
            "3078/3078 [==============================] - 1364s 443ms/step - loss: 3.9659 - val_loss: 5.0549 - lr: 2.0000e-04\n",
            "Epoch 12/20\n",
            "3078/3078 [==============================] - 1375s 447ms/step - loss: 3.9035 - val_loss: 5.0527 - lr: 2.0000e-04\n",
            "Epoch 13/20\n",
            "3078/3078 [==============================] - 1379s 448ms/step - loss: 3.8674 - val_loss: 5.0628 - lr: 1.0000e-04\n",
            "Epoch 14/20\n",
            "3078/3078 [==============================] - 1370s 445ms/step - loss: 3.8459 - val_loss: 5.0765 - lr: 1.0000e-04\n",
            "Epoch 15/20\n",
            "3078/3078 [==============================] - 1386s 450ms/step - loss: 3.8255 - val_loss: 5.1292 - lr: 1.0000e-04\n",
            "Epoch 16/20\n",
            "3078/3078 [==============================] - 1396s 454ms/step - loss: 3.7846 - val_loss: 5.1196 - lr: 1.0000e-04\n",
            "Epoch 17/20\n",
            "3078/3078 [==============================] - 1371s 445ms/step - loss: 3.7758 - val_loss: 5.1533 - lr: 1.0000e-04\n",
            "Epoch 18/20\n",
            "3078/3078 [==============================] - 1398s 454ms/step - loss: 3.7440 - val_loss: 5.1355 - lr: 1.0000e-04\n",
            "Epoch 19/20\n",
            "3078/3078 [==============================] - 1376s 447ms/step - loss: 3.7275 - val_loss: 5.1552 - lr: 1.0000e-04\n",
            "Epoch 20/20\n",
            "3078/3078 [==============================] - 1390s 452ms/step - loss: 3.7076 - val_loss: 5.2022 - lr: 1.0000e-04\n",
            "CPU times: user 14h 16min 34s, sys: 19min 22s, total: 14h 35min 56s\n",
            "Wall time: 7h 44min 13s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o4IUMwLfQAg9"
      },
      "source": [
        "## Load Model, Vocab and Embedded if already trained "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9515a036-a921-4f7d-8cc4-208c95d6599e",
        "id": "EP1CvXdVQAg-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "model = tf.keras.models.load_model('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/ActivationFun_BidirectionalModel/Elu/Business(LSTM).20-3.7076.h5')\n",
        "\n",
        "# Read Vocabulary of data\n",
        "vocab = []\n",
        "\n",
        "with open('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/LSTM_model/DocsToLoad/vocab.data', 'rb') as filehandle:\n",
        "    # read the data as binary data stream\n",
        "    vocab = pickle.load(filehandle)\n",
        "    \n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5567 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c97387cb-570a-4561-95e9-da7fe330b752",
        "id": "TP7G2SO9QAhE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Read embedded spaces of vocab data\n",
        "\n",
        "embedded = np.load('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/LSTM_model/DocsToLoad/embedded.npy')\n",
        "\n",
        "print(embedded.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5567, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vM9vRbd7QAhH"
      },
      "source": [
        "## Evaluate Model Elu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LF0Dp2fAQAhI",
        "colab": {}
      },
      "source": [
        "def generateInputArray(inputs):\n",
        "  embDim = embedded.shape[1]\n",
        "  x_sample =[]\n",
        "  for x in inputs:\n",
        "    if x in vocab:\n",
        "      x_sample.append(embedded[list(vocab).index(x)])\n",
        "    else:\n",
        "      x_sample.append(np.zeros(embDim))\n",
        "\n",
        "  x_sample = np.array(x_sample)\n",
        "  x_sample = np.expand_dims(x_sample, axis=0)\n",
        "  return(x_sample)\n",
        "  \n",
        "def generatecandidates(text):\n",
        "  textToken = tf.keras.preprocessing.text.text_to_word_sequence(str(text),  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r0123456789'+\"'\")   \n",
        "  \n",
        "  x_sample = generateInputArray(textToken[-5:])\n",
        "  \n",
        "  y_sample = model.predict(x_sample)\n",
        "  \n",
        "  # Get top 50 candidates\n",
        "  ind = np.argpartition(y_sample[0,:], -50)[-50:]\n",
        "\n",
        "  candidates=dict()\n",
        "  for i in ind:\n",
        "    if vocab[i] != \"<Unkown>\":\n",
        "      candidates[vocab[i]] = y_sample[0,i]\n",
        "\n",
        "  return(candidates)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "teZRFL4XQAhM",
        "colab": {}
      },
      "source": [
        "X_line, Y_line = generate_text_sequences(TestLines, pastWords, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "85a574b1-2596-4f46-b2db-6e5179afb763",
        "id": "Iod3BI2hQAhP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "points=0\n",
        "TotalPredictions = 0\n",
        "for i in range(0, len(TestLines)):\n",
        "    # Verify if the correct word is in the top-50 words predicted\n",
        "    candidates = generatecandidates(X_line[i])\n",
        "    if Y_line[i] == '<Unkown>':\n",
        "        continue\n",
        "    else:\n",
        "        if Y_line[i] in candidates.keys():\n",
        "            word_prob = list(candidates.keys()).index(Y_line[i])\n",
        "            p = np.array(list(candidates.values()))\n",
        "            points += 1 - p[word_prob] \n",
        "            TotalPredictions +=1\n",
        "        else:\n",
        "            points+=1\n",
        "            TotalPredictions +=1\n",
        "\n",
        "print(\"Average uncertanty of the model \", (points/TotalPredictions)*100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average uncertanty of the model  82.25302414812074 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CxtPkhSkvwQ",
        "colab_type": "code",
        "outputId": "e02ba38a-3940-4e4d-8232-bd6a96b71648",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "points=0\n",
        "TotalPredictions = 0\n",
        "\n",
        "for i in range(0, len(TestLines)):\n",
        "    # Verify if the correct word is in the words predicted\n",
        "    candidates = generatecandidates(X_line[i])\n",
        "    if Y_line[i] == '<Unkown>':\n",
        "        continue\n",
        "    else:\n",
        "        if Y_line[i] in candidates.keys():\n",
        "            points+=1\n",
        "            TotalPredictions +=1\n",
        "        else:\n",
        "            TotalPredictions +=1\n",
        "    \n",
        "    \n",
        "print(\"Percentage of times next word was on top 50 predictions: \",(points/TotalPredictions)*100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of times next word was on top 50 predictions:  61.66502138861467 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UCuCYspPNabL"
      },
      "source": [
        "## Create Model  Sigmoid\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d2c7592c-d003-467a-d4f6-990b40c069f3",
        "id": "IJ7vCN1KNabM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, GRU, LSTM, Bidirectional\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from keras import losses\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=500, return_sequences=True,input_shape=(pastWords,embDim), activation='sigmoid'))\n",
        "model.add(LSTM(units=200))\n",
        "model.add(Dense(len(vocab), activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 5, 500)            1202000   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 200)               560800    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5567)              1118967   \n",
            "=================================================================\n",
            "Total params: 2,881,767\n",
            "Trainable params: 2,881,767\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OrBXkFYoNabV",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "optimizer = RMSprop(lr=0.001)\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "model_file = \"/content/drive/My Drive/SharpestMinds/3. Generate Language Model/ActivationFun_BidirectionalModel/sigmoid/Business(LSTM).{epoch:02d}-{loss:.4f}.h5\"\n",
        "\n",
        "mc = ModelCheckpoint(model_file, monitor=\"loss\", mode=\"min\", save_best_only=True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=4, min_lr=1e-4)\n",
        "\n",
        "es = EarlyStopping(monitor='loss', min_delta=0.01, patience=5, mode='min')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3bb89a29-18ff-4ca8-f883-1e7240410833",
        "id": "TX6LOTPyNabZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 787
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "history = model.fit_generator(generator=generator,\n",
        "                    epochs=20,\n",
        "                    steps_per_epoch= len(X_lineTrain)//batchsize,\n",
        "                    validation_data=valgenerator,\n",
        "                    validation_steps= len(X_lineTest)//batchsize,\n",
        "                    callbacks=[mc, reduce_lr, es])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <timed exec>:7: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/20\n",
            "3078/3078 [==============================] - 1370s 445ms/step - loss: 5.8851 - val_loss: 5.4511 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "3078/3078 [==============================] - 1372s 446ms/step - loss: 5.2522 - val_loss: 5.1906 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "3078/3078 [==============================] - 1364s 443ms/step - loss: 5.0240 - val_loss: 5.0915 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "3078/3078 [==============================] - 1379s 448ms/step - loss: 4.9123 - val_loss: 5.0513 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "3078/3078 [==============================] - 1362s 443ms/step - loss: 4.8460 - val_loss: 4.9944 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "3078/3078 [==============================] - 1367s 444ms/step - loss: 4.7686 - val_loss: 4.9470 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "3078/3078 [==============================] - 1378s 448ms/step - loss: 4.7193 - val_loss: 5.0834 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "3078/3078 [==============================] - 1364s 443ms/step - loss: 4.6650 - val_loss: 4.9858 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "3078/3078 [==============================] - 1376s 447ms/step - loss: 4.6275 - val_loss: 4.9367 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "3078/3078 [==============================] - 1369s 445ms/step - loss: 4.5802 - val_loss: 4.9582 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "3078/3078 [==============================] - 1362s 443ms/step - loss: 4.5293 - val_loss: 4.9227 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "3078/3078 [==============================] - 1384s 450ms/step - loss: 4.5028 - val_loss: 4.9479 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "3078/3078 [==============================] - 1371s 445ms/step - loss: 4.4777 - val_loss: 4.9475 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "3078/3078 [==============================] - 1379s 448ms/step - loss: 4.4586 - val_loss: 4.9690 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "3078/3078 [==============================] - 1378s 448ms/step - loss: 4.4377 - val_loss: 5.0189 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "3078/3078 [==============================] - 1375s 447ms/step - loss: 4.2832 - val_loss: 4.9065 - lr: 2.0000e-04\n",
            "Epoch 17/20\n",
            "3078/3078 [==============================] - 1382s 449ms/step - loss: 4.2483 - val_loss: 4.9105 - lr: 2.0000e-04\n",
            "Epoch 18/20\n",
            "3078/3078 [==============================] - 1375s 447ms/step - loss: 4.2218 - val_loss: 4.9200 - lr: 2.0000e-04\n",
            "Epoch 19/20\n",
            "3078/3078 [==============================] - 1383s 449ms/step - loss: 4.2201 - val_loss: 4.9438 - lr: 2.0000e-04\n",
            "Epoch 20/20\n",
            "3078/3078 [==============================] - 1380s 448ms/step - loss: 4.2071 - val_loss: 4.9301 - lr: 2.0000e-04\n",
            "CPU times: user 14h 15min 3s, sys: 18min 17s, total: 14h 33min 20s\n",
            "Wall time: 7h 38min 3s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cQYNiOwdNabg"
      },
      "source": [
        "## Load Model, Vocab and Embedded if already trained "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a144a43e-1d42-46fb-bb93-05990e0681cc",
        "id": "LP3Y8XuaNabg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "model = tf.keras.models.load_model('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/ActivationFun_BidirectionalModel/sigmoid/Business(LSTM).20-4.2071.h5')\n",
        "\n",
        "# Read Vocabulary of data\n",
        "vocab = []\n",
        "\n",
        "with open('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/LSTM_model/DocsToLoad/vocab.data', 'rb') as filehandle:\n",
        "    # read the data as binary data stream\n",
        "    vocab = pickle.load(filehandle)\n",
        "    \n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5567 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "35fdf28b-383b-4b8d-a8b9-a0f0bf90430d",
        "id": "gH9k1LzjNabj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Read embedded spaces of vocab data\n",
        "\n",
        "embedded = np.load('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/LSTM_model/DocsToLoad/embedded.npy')\n",
        "\n",
        "print(embedded.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5567, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2XgJKOWINabm"
      },
      "source": [
        "## Evaluate Model Sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oHA89HlpNabm",
        "colab": {}
      },
      "source": [
        "def generateInputArray(inputs):\n",
        "  embDim = embedded.shape[1]\n",
        "  x_sample =[]\n",
        "  for x in inputs:\n",
        "    if x in vocab:\n",
        "      x_sample.append(embedded[list(vocab).index(x)])\n",
        "    else:\n",
        "      x_sample.append(np.zeros(embDim))\n",
        "\n",
        "  x_sample = np.array(x_sample)\n",
        "  x_sample = np.expand_dims(x_sample, axis=0)\n",
        "  return(x_sample)\n",
        "  \n",
        "def generatecandidates(text):\n",
        "  textToken = tf.keras.preprocessing.text.text_to_word_sequence(str(text),  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r0123456789'+\"'\")   \n",
        "  \n",
        "  x_sample = generateInputArray(textToken[-5:])\n",
        "  \n",
        "  y_sample = model.predict(x_sample)\n",
        "  \n",
        "  # Get top 50 candidates\n",
        "  ind = np.argpartition(y_sample[0,:], -50)[-50:]\n",
        "\n",
        "  candidates=dict()\n",
        "  for i in ind:\n",
        "    if vocab[i] != \"<Unkown>\":\n",
        "      candidates[vocab[i]] = y_sample[0,i]\n",
        "\n",
        "  return(candidates)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WFw4LWyhNabp",
        "colab": {}
      },
      "source": [
        "X_line, Y_line = generate_text_sequences(TestLines, pastWords, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "efV8w6QCNabt",
        "outputId": "e9eea304-76c7-4593-ba16-b917ad452d53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "points=0\n",
        "TotalPredictions = 0\n",
        "for i in range(0, len(TestLines)):\n",
        "    # Verify if the correct word is in the top-50 words predicted\n",
        "    candidates = generatecandidates(X_line[i])\n",
        "    if Y_line[i] == '<Unkown>':\n",
        "        continue\n",
        "    else:\n",
        "        if Y_line[i] in candidates.keys():\n",
        "            word_prob = list(candidates.keys()).index(Y_line[i])\n",
        "            p = np.array(list(candidates.values()))\n",
        "            points += 1 - p[word_prob] \n",
        "            TotalPredictions +=1\n",
        "        else:\n",
        "            points+=1\n",
        "            TotalPredictions +=1\n",
        "\n",
        "print(\"Average uncertanty of the model \", (points/TotalPredictions)*100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average uncertanty of the model  84.25173470110818 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK2pWzrXla5C",
        "colab_type": "code",
        "outputId": "cfe6dae6-ad6a-4f59-b4f9-c1f33fd2a559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "points=0\n",
        "TotalPredictions = 0\n",
        "\n",
        "for i in range(0, len(TestLines)):\n",
        "    # Verify if the correct word is in the words predicted\n",
        "    candidates = generatecandidates(X_line[i])\n",
        "    if Y_line[i] == '<Unkown>':\n",
        "        continue\n",
        "    else:\n",
        "        if Y_line[i] in candidates.keys():\n",
        "            points+=1\n",
        "            TotalPredictions +=1\n",
        "        else:\n",
        "            TotalPredictions +=1\n",
        "    \n",
        "    \n",
        "print(\"Percentage of times next word was on top 50 predictions: \",(points/TotalPredictions)*100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of times next word was on top 50 predictions:  65.2846331029944 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ql49jFtscWG7"
      },
      "source": [
        "## Create Model  Softsign\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9ac78a8b-ba27-45e1-ec08-2aff8fd9ea9d",
        "id": "O5_1w792cWG8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, GRU, LSTM, Bidirectional\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from keras import losses\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=500, return_sequences=True,input_shape=(pastWords,embDim), activation='softsign'))\n",
        "model.add(LSTM(units=200))\n",
        "model.add(Dense(len(vocab), activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 5, 500)            1202000   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 200)               560800    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5567)              1118967   \n",
            "=================================================================\n",
            "Total params: 2,881,767\n",
            "Trainable params: 2,881,767\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "247mAAyDcWHA",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "optimizer = RMSprop(lr=0.001)\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "model_file = \"/content/drive/My Drive/SharpestMinds/3. Generate Language Model/ActivationFun_BidirectionalModel/softsign/Business(LSTM).{epoch:02d}-{loss:.4f}.h5\"\n",
        "\n",
        "mc = ModelCheckpoint(model_file, monitor=\"loss\", mode=\"min\", save_best_only=True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=4, min_lr=1e-4)\n",
        "\n",
        "es = EarlyStopping(monitor='loss', min_delta=0.01, patience=5, mode='min')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ec13961f-1e4c-4ba5-a523-80245467cada",
        "id": "2iVwpzZFcWHD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 787
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "history = model.fit_generator(generator=generator,\n",
        "                    epochs=20,\n",
        "                    steps_per_epoch= len(X_lineTrain)//batchsize,\n",
        "                    validation_data=valgenerator,\n",
        "                    validation_steps= len(X_lineTest)//batchsize,\n",
        "                    callbacks=[mc, reduce_lr, es])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <timed exec>:7: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/20\n",
            "3078/3078 [==============================] - 1442s 469ms/step - loss: 5.6911 - val_loss: 5.2949 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "3078/3078 [==============================] - 1449s 471ms/step - loss: 5.0423 - val_loss: 5.0625 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "3078/3078 [==============================] - 1441s 468ms/step - loss: 4.8349 - val_loss: 4.9428 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "3078/3078 [==============================] - 1438s 467ms/step - loss: 4.6777 - val_loss: 4.9253 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "3078/3078 [==============================] - 1443s 469ms/step - loss: 4.5555 - val_loss: 4.9079 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "3078/3078 [==============================] - 1437s 467ms/step - loss: 4.5021 - val_loss: 4.9358 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "3078/3078 [==============================] - 1435s 466ms/step - loss: 4.4020 - val_loss: 4.9398 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "3078/3078 [==============================] - 1447s 470ms/step - loss: 4.3114 - val_loss: 4.9022 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "3078/3078 [==============================] - 1439s 468ms/step - loss: 4.2563 - val_loss: 4.9706 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "3078/3078 [==============================] - 1438s 467ms/step - loss: 4.2272 - val_loss: 5.0271 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "3078/3078 [==============================] - 1451s 471ms/step - loss: 4.1768 - val_loss: 5.0039 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "3078/3078 [==============================] - 1442s 469ms/step - loss: 4.1626 - val_loss: 5.0525 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "3078/3078 [==============================] - 1446s 470ms/step - loss: 4.0686 - val_loss: 5.0275 - lr: 2.0000e-04\n",
            "Epoch 14/20\n",
            "3078/3078 [==============================] - 1433s 466ms/step - loss: 4.0244 - val_loss: 5.0692 - lr: 2.0000e-04\n",
            "Epoch 15/20\n",
            "3078/3078 [==============================] - 1447s 470ms/step - loss: 3.9816 - val_loss: 5.0870 - lr: 2.0000e-04\n",
            "Epoch 16/20\n",
            "3078/3078 [==============================] - 1450s 471ms/step - loss: 3.9503 - val_loss: 5.0474 - lr: 2.0000e-04\n",
            "Epoch 17/20\n",
            "3078/3078 [==============================] - 1463s 475ms/step - loss: 3.8856 - val_loss: 5.0528 - lr: 1.0000e-04\n",
            "Epoch 18/20\n",
            "3078/3078 [==============================] - 1457s 473ms/step - loss: 3.8890 - val_loss: 5.0683 - lr: 1.0000e-04\n",
            "Epoch 19/20\n",
            "3078/3078 [==============================] - 1456s 473ms/step - loss: 3.8591 - val_loss: 5.0762 - lr: 1.0000e-04\n",
            "Epoch 20/20\n",
            "3078/3078 [==============================] - 1464s 476ms/step - loss: 3.8503 - val_loss: 5.1356 - lr: 1.0000e-04\n",
            "CPU times: user 15h 1min 26s, sys: 20min 4s, total: 15h 21min 31s\n",
            "Wall time: 8h 2min 10s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lfhQXXKqcWHG"
      },
      "source": [
        "## Load Model, Vocab and Embedded if already trained "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "36efb311-32f8-4a65-c7d8-34afdd1b276f",
        "id": "XqCvKcRNcWHG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "model = tf.keras.models.load_model('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/ActivationFun_BidirectionalModel/softsign/Business(LSTM).20-3.8503.h5')\n",
        "\n",
        "# Read Vocabulary of data\n",
        "vocab = []\n",
        "\n",
        "with open('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/LSTM_model/DocsToLoad/vocab.data', 'rb') as filehandle:\n",
        "    # read the data as binary data stream\n",
        "    vocab = pickle.load(filehandle)\n",
        "    \n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5567 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f3b4d9f8-fc45-4806-8431-f876e78fd0a9",
        "id": "7RebcCrNcWHJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Read embedded spaces of vocab data\n",
        "\n",
        "embedded = np.load('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/LSTM_model/DocsToLoad/embedded.npy')\n",
        "\n",
        "print(embedded.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5567, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZDD7E7fmcWHM"
      },
      "source": [
        "## Evaluate Model Softsign"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iCrBcyqRcWHN",
        "colab": {}
      },
      "source": [
        "def generateInputArray(inputs):\n",
        "  embDim = embedded.shape[1]\n",
        "  x_sample =[]\n",
        "  for x in inputs:\n",
        "    if x in vocab:\n",
        "      x_sample.append(embedded[list(vocab).index(x)])\n",
        "    else:\n",
        "      x_sample.append(np.zeros(embDim))\n",
        "\n",
        "  x_sample = np.array(x_sample)\n",
        "  x_sample = np.expand_dims(x_sample, axis=0)\n",
        "  return(x_sample)\n",
        "  \n",
        "def generatecandidates(text):\n",
        "  textToken = tf.keras.preprocessing.text.text_to_word_sequence(str(text),  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r0123456789'+\"'\")   \n",
        "  \n",
        "  x_sample = generateInputArray(textToken[-5:])\n",
        "  \n",
        "  y_sample = model.predict(x_sample)\n",
        "  \n",
        "  # Get top 50 candidates\n",
        "  ind = np.argpartition(y_sample[0,:], -50)[-50:]\n",
        "\n",
        "  candidates=dict()\n",
        "  for i in ind:\n",
        "    if vocab[i] != \"<Unkown>\":\n",
        "      candidates[vocab[i]] = y_sample[0,i]\n",
        "\n",
        "  return(candidates)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wafVcJencWHS",
        "colab": {}
      },
      "source": [
        "X_line, Y_line = generate_text_sequences(TestLines, pastWords, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9685045b-05ff-42ce-a601-a8892e15dd42",
        "id": "OqEPjqXIcWHW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "points=0\n",
        "TotalPredictions = 0\n",
        "for i in range(0, len(TestLines)):\n",
        "    # Verify if the correct word is in the top-50 words predicted\n",
        "    candidates = generatecandidates(X_line[i])\n",
        "    if Y_line[i] == '<Unkown>':\n",
        "        continue\n",
        "    else:\n",
        "        if Y_line[i] in candidates.keys():\n",
        "            word_prob = list(candidates.keys()).index(Y_line[i])\n",
        "            p = np.array(list(candidates.values()))\n",
        "            points += 1 - p[word_prob] \n",
        "            TotalPredictions +=1\n",
        "        else:\n",
        "            points+=1\n",
        "            TotalPredictions +=1\n",
        "\n",
        "print(\"Average uncertanty of the model \", (points/TotalPredictions)*100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average uncertanty of the model  82.62568302048687 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W9tQPXh7NacM",
        "outputId": "df221884-38d8-4b10-bde7-379b003d334e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "points=0\n",
        "TotalPredictions = 0\n",
        "\n",
        "for i in range(0, len(TestLines)):\n",
        "    # Verify if the correct word is in the words predicted\n",
        "    candidates = generatecandidates(X_line[i])\n",
        "    if Y_line[i] == '<Unkown>':\n",
        "        continue\n",
        "    else:\n",
        "        if Y_line[i] in candidates.keys():\n",
        "            points+=1\n",
        "            TotalPredictions +=1\n",
        "        else:\n",
        "            TotalPredictions +=1\n",
        "    \n",
        "    \n",
        "print(\"Percentage of times next word was on top 50 predictions: \",(points/TotalPredictions)*100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of times next word was on top 50 predictions:  61.99407699901284 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o4tIIv18t18B"
      },
      "source": [
        "## Evaluate Model Tanh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7b0dbd06-f787-41ae-cd86-3a64e2d0bc30",
        "id": "gDhC8z-zt18E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "model = tf.keras.models.load_model('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/Bidirectional_LSTM_model/train_log/simpleTextGenerator.19-4.10.h5')\n",
        "\n",
        "# Read Vocabulary of data\n",
        "vocab = []\n",
        "\n",
        "with open('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/LSTM_model/DocsToLoad/vocab.data', 'rb') as filehandle:\n",
        "    # read the data as binary data stream\n",
        "    vocab = pickle.load(filehandle)\n",
        "    \n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5567 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0171bfb8-cdfa-48f6-9042-8a3fe4672fa7",
        "id": "haeS06tEt18I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Read embedded spaces of vocab data\n",
        "\n",
        "embedded = np.load('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/LSTM_model/DocsToLoad/embedded.npy')\n",
        "\n",
        "print(embedded.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5567, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_Vgn9Wkqt18N",
        "colab": {}
      },
      "source": [
        "def generateInputArray(inputs):\n",
        "  embDim = embedded.shape[1]\n",
        "  x_sample =[]\n",
        "  for x in inputs:\n",
        "    if x in vocab:\n",
        "      x_sample.append(embedded[list(vocab).index(x)])\n",
        "    else:\n",
        "      x_sample.append(np.zeros(embDim))\n",
        "\n",
        "  x_sample = np.array(x_sample)\n",
        "  x_sample = np.expand_dims(x_sample, axis=0)\n",
        "  return(x_sample)\n",
        "  \n",
        "def generatecandidates(text):\n",
        "  textToken = tf.keras.preprocessing.text.text_to_word_sequence(str(text),  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r0123456789'+\"'\")   \n",
        "  \n",
        "  x_sample = generateInputArray(textToken[-5:])\n",
        "  \n",
        "  y_sample = model.predict(x_sample)\n",
        "  \n",
        "  # Get top 50 candidates\n",
        "  ind = np.argpartition(y_sample[0,:], -50)[-50:]\n",
        "\n",
        "  candidates=dict()\n",
        "  for i in ind:\n",
        "    if vocab[i] != \"<Unkown>\":\n",
        "      candidates[vocab[i]] = y_sample[0,i]\n",
        "\n",
        "  return(candidates)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uwe8ywMHt18S",
        "colab": {}
      },
      "source": [
        "X_line, Y_line = generate_text_sequences(TestLines, pastWords, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b2d22b68-1d5d-42b5-d78f-d2bbec5fcc45",
        "id": "nUudcxVWt18W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "points=0\n",
        "TotalPredictions = 0\n",
        "for i in range(0, len(TestLines)):\n",
        "    # Verify if the correct word is in the top-50 words predicted\n",
        "    candidates = generatecandidates(X_line[i])\n",
        "    if Y_line[i] == '<Unkown>':\n",
        "        continue\n",
        "    else:\n",
        "        if Y_line[i] in candidates.keys():\n",
        "            word_prob = list(candidates.keys()).index(Y_line[i])\n",
        "            p = np.array(list(candidates.values()))\n",
        "            points += 1 - p[word_prob] \n",
        "            TotalPredictions +=1\n",
        "        else:\n",
        "            points+=1\n",
        "            TotalPredictions +=1\n",
        "\n",
        "print(\"Average uncertanty of the model \", (points/TotalPredictions)*100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average uncertanty of the model  83.31115681597 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YvlquLUnt18d",
        "outputId": "f17bf83d-b028-4f14-872c-8e6321154780",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "points=0\n",
        "TotalPredictions = 0\n",
        "\n",
        "for i in range(0, len(TestLines)):\n",
        "    # Verify if the correct word is in the words predicted\n",
        "    candidates = generatecandidates(X_line[i])\n",
        "    if Y_line[i] == '<Unkown>':\n",
        "        continue\n",
        "    else:\n",
        "        if Y_line[i] in candidates.keys():\n",
        "            points+=1\n",
        "            TotalPredictions +=1\n",
        "        else:\n",
        "            TotalPredictions +=1\n",
        "    \n",
        "    \n",
        "print(\"Percentage of times next word was on top 50 predictions: \",(points/TotalPredictions)*100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of times next word was on top 50 predictions:  65.15301085883515 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CGR_LNHyq1oR"
      },
      "source": [
        "## Evaluate Model Unidirectional Tanh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "78990870-575d-47b8-8c4e-485d1d9214f3",
        "id": "cUkFZBoQs5oN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "model = tf.keras.models.load_model('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/LSTM_model/train_log/simpleTextGenerator(LSTM).20-5.1207.h5')\n",
        "\n",
        "# Read Vocabulary of data\n",
        "vocab = []\n",
        "\n",
        "with open('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/LSTM_model/DocsToLoad/vocab.data', 'rb') as filehandle:\n",
        "    # read the data as binary data stream\n",
        "    vocab = pickle.load(filehandle)\n",
        "    \n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5567 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "bc91a52b-46af-4d8c-f052-3863aed2e998",
        "id": "vmbvj2NWs5oR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Read embedded spaces of vocab data\n",
        "\n",
        "embedded = np.load('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/LSTM_model/DocsToLoad/embedded.npy')\n",
        "\n",
        "print(embedded.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5567, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hLP4wZs-q1oT",
        "colab": {}
      },
      "source": [
        "def generateInputArray(inputs):\n",
        "  embDim = embedded.shape[1]\n",
        "  x_sample =[]\n",
        "  for x in inputs:\n",
        "    if x in vocab:\n",
        "      x_sample.append(embedded[list(vocab).index(x)])\n",
        "    else:\n",
        "      x_sample.append(np.zeros(embDim))\n",
        "\n",
        "  x_sample = np.array(x_sample)\n",
        "  x_sample = np.expand_dims(x_sample, axis=0)\n",
        "  return(x_sample)\n",
        "  \n",
        "def generatecandidates(text):\n",
        "  textToken = tf.keras.preprocessing.text.text_to_word_sequence(str(text),  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r0123456789'+\"'\")   \n",
        "  \n",
        "  x_sample = generateInputArray(textToken[-5:])\n",
        "  \n",
        "  y_sample = model.predict(x_sample)\n",
        "  \n",
        "  # Get top 50 candidates\n",
        "  ind = np.argpartition(y_sample[0,:], -50)[-50:]\n",
        "\n",
        "  candidates=dict()\n",
        "  for i in ind:\n",
        "    if vocab[i] != \"<Unkown>\":\n",
        "      candidates[vocab[i]] = y_sample[0,i]\n",
        "\n",
        "  return(candidates)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pps2FMOXq1oa",
        "colab": {}
      },
      "source": [
        "X_line, Y_line = generate_text_sequences(TestLines, pastWords, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f4610bcf-3bd3-4047-95ca-14e976617558",
        "id": "nV-Upjusq1od",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "points=0\n",
        "TotalPredictions = 0\n",
        "for i in range(0, len(TestLines)):\n",
        "    # Verify if the correct word is in the top-50 words predicted\n",
        "    candidates = generatecandidates(X_line[i])\n",
        "    if Y_line[i] == '<Unkown>':\n",
        "        continue\n",
        "    else:\n",
        "        if Y_line[i] in candidates.keys():\n",
        "            word_prob = list(candidates.keys()).index(Y_line[i])\n",
        "            p = np.array(list(candidates.values()))\n",
        "            points += 1 - p[word_prob] \n",
        "            TotalPredictions +=1\n",
        "        else:\n",
        "            points+=1\n",
        "            TotalPredictions +=1\n",
        "\n",
        "print(\"Average uncertanty of the model \", (points/TotalPredictions)*100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average uncertanty of the model  82.81953041101032 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8f64bd11-aed1-454c-fe3e-81b374f81244",
        "id": "5LekEdGGq1oh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "points=0\n",
        "TotalPredictions = 0\n",
        "\n",
        "for i in range(0, len(TestLines)):\n",
        "    # Verify if the correct word is in the words predicted\n",
        "    candidates = generatecandidates(X_line[i])\n",
        "    if Y_line[i] == '<Unkown>':\n",
        "        continue\n",
        "    else:\n",
        "        if Y_line[i] in candidates.keys():\n",
        "            points+=1\n",
        "            TotalPredictions +=1\n",
        "        else:\n",
        "            TotalPredictions +=1\n",
        "    \n",
        "    \n",
        "print(\"Percentage of times next word was on top 50 predictions: \",(points/TotalPredictions)*100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of times next word was on top 50 predictions:  61.467588022375786 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4t-nEjQdKRFq"
      },
      "source": [
        "## Evaluate Model Bidirectional Tanh final Epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d7d74481-faa3-4bdc-ce0c-ab33c102cb1f",
        "id": "oSDO03oNKRFt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "model = tf.keras.models.load_model('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/Bidirectional_LSTM_model/train_log/simpleTextGenerator.56-3.08.h5')\n",
        "\n",
        "# Read Vocabulary of data\n",
        "vocab = []\n",
        "\n",
        "with open('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/LSTM_model/DocsToLoad/vocab.data', 'rb') as filehandle:\n",
        "    # read the data as binary data stream\n",
        "    vocab = pickle.load(filehandle)\n",
        "    \n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5567 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4a2428e3-ed07-4ef7-eb93-4186a130e4bf",
        "id": "R-Owc_7UKRF6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Read embedded spaces of vocab data\n",
        "\n",
        "embedded = np.load('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/LSTM_model/DocsToLoad/embedded.npy')\n",
        "\n",
        "print(embedded.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5567, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GHPjTuxvKRGB",
        "colab": {}
      },
      "source": [
        "def generateInputArray(inputs):\n",
        "  embDim = embedded.shape[1]\n",
        "  x_sample =[]\n",
        "  for x in inputs:\n",
        "    if x in vocab:\n",
        "      x_sample.append(embedded[list(vocab).index(x)])\n",
        "    else:\n",
        "      x_sample.append(np.zeros(embDim))\n",
        "\n",
        "  x_sample = np.array(x_sample)\n",
        "  x_sample = np.expand_dims(x_sample, axis=0)\n",
        "  return(x_sample)\n",
        "  \n",
        "def generatecandidates(text):\n",
        "  textToken = tf.keras.preprocessing.text.text_to_word_sequence(str(text),  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r0123456789'+\"'\")   \n",
        "  \n",
        "  x_sample = generateInputArray(textToken[-5:])\n",
        "  \n",
        "  y_sample = model.predict(x_sample)\n",
        "  \n",
        "  # Get top 50 candidates\n",
        "  ind = np.argpartition(y_sample[0,:], -50)[-50:]\n",
        "\n",
        "  candidates=dict()\n",
        "  for i in ind:\n",
        "    if vocab[i] != \"<Unkown>\":\n",
        "      candidates[vocab[i]] = y_sample[0,i]\n",
        "\n",
        "  return(candidates)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rp_qqLd6KRGI",
        "colab": {}
      },
      "source": [
        "X_line, Y_line = generate_text_sequences(TestLines, pastWords, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "aeb4f520-914b-4829-db15-80e8ddc536c9",
        "id": "dVosJOX1KRGM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "points=0\n",
        "TotalPredictions = 0\n",
        "for i in range(0, len(TestLines)):\n",
        "    # Verify if the correct word is in the top-50 words predicted\n",
        "    candidates = generatecandidates(X_line[i])\n",
        "    if Y_line[i] == '<Unkown>':\n",
        "        continue\n",
        "    else:\n",
        "        if Y_line[i] in candidates.keys():\n",
        "            word_prob = list(candidates.keys()).index(Y_line[i])\n",
        "            p = np.array(list(candidates.values()))\n",
        "            points += 1 - p[word_prob] \n",
        "            TotalPredictions +=1\n",
        "        else:\n",
        "            points+=1\n",
        "            TotalPredictions +=1\n",
        "\n",
        "print(\"Average uncertainty of the model \", (points/TotalPredictions)*100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average uncertanty of the model  80.073489672101 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2223aa5a-d922-48b8-821d-d1ae8e219d78",
        "id": "h6Si7_ZIKRGR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "points=0\n",
        "TotalPredictions = 0\n",
        "\n",
        "for i in range(0, len(TestLines)):\n",
        "    # Verify if the correct word is in the top-50 words predicted\n",
        "    candidates = generatecandidates(X_line[i])\n",
        "    if Y_line[i] == '<Unkown>':\n",
        "        continue\n",
        "    else:\n",
        "        if Y_line[i] in candidates.keys():\n",
        "            points+=1\n",
        "            TotalPredictions +=1\n",
        "        else:\n",
        "            TotalPredictions +=1\n",
        "    \n",
        "    \n",
        "print(\"Percentage of times next word was on top 50 predictions: \",(points/TotalPredictions)*100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of times next word was on top 50 predictions:  60.97400460677854 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nPhqmi9OTAnK"
      },
      "source": [
        "## Evaluate Model Unidirectional Tanh final Epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "377fbe7c-9e67-4a59-88ac-e72206a16933",
        "id": "T6IhhpWfTAnO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "model = tf.keras.models.load_model('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/LSTM_model/train_log/simpleTextGenerator(LSTM).56-5.6304.h5')\n",
        "\n",
        "# Read Vocabulary of data\n",
        "vocab = []\n",
        "\n",
        "with open('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/LSTM_model/DocsToLoad/vocab.data', 'rb') as filehandle:\n",
        "    # read the data as binary data stream\n",
        "    vocab = pickle.load(filehandle)\n",
        "    \n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5567 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7f8d0e28-1067-495f-fa43-c75488a6e27a",
        "id": "PaYO0AhkTAng",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Read embedded spaces of vocab data\n",
        "\n",
        "embedded = np.load('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/LSTM_model/DocsToLoad/embedded.npy')\n",
        "\n",
        "print(embedded.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5567, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yl4DNebDTAnt",
        "colab": {}
      },
      "source": [
        "def generateInputArray(inputs):\n",
        "  embDim = embedded.shape[1]\n",
        "  x_sample =[]\n",
        "  for x in inputs:\n",
        "    if x in vocab:\n",
        "      x_sample.append(embedded[list(vocab).index(x)])\n",
        "    else:\n",
        "      x_sample.append(np.zeros(embDim))\n",
        "\n",
        "  x_sample = np.array(x_sample)\n",
        "  x_sample = np.expand_dims(x_sample, axis=0)\n",
        "  return(x_sample)\n",
        "  \n",
        "def generatecandidates(text):\n",
        "  textToken = tf.keras.preprocessing.text.text_to_word_sequence(str(text),  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r0123456789'+\"'\")   \n",
        "  \n",
        "  x_sample = generateInputArray(textToken[-5:])\n",
        "  \n",
        "  y_sample = model.predict(x_sample)\n",
        "  \n",
        "  # Get top 50 candidates\n",
        "  ind = np.argpartition(y_sample[0,:], -50)[-50:]\n",
        "\n",
        "  candidates=dict()\n",
        "  for i in ind:\n",
        "    if vocab[i] != \"<Unkown>\":\n",
        "      candidates[vocab[i]] = y_sample[0,i]\n",
        "\n",
        "  return(candidates)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E_IlgSxFTAn2",
        "colab": {}
      },
      "source": [
        "X_line, Y_line = generate_text_sequences(TestLines, pastWords, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "75f98f05-efe2-4187-8c98-d4ca18b6f9e7",
        "id": "5jZZ_xcCTAn4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "points=0\n",
        "TotalPredictions = 0\n",
        "for i in range(0, len(TestLines)):\n",
        "    # Verify if the correct word is in the top-50 words predicted\n",
        "    candidates = generatecandidates(X_line[i])\n",
        "    if Y_line[i] == '<Unkown>':\n",
        "        continue\n",
        "    else:\n",
        "        if Y_line[i] in candidates.keys():\n",
        "            word_prob = list(candidates.keys()).index(Y_line[i])\n",
        "            p = np.array(list(candidates.values()))\n",
        "            points += 1 - p[word_prob] \n",
        "            TotalPredictions +=1\n",
        "        else:\n",
        "            points+=1\n",
        "            TotalPredictions +=1\n",
        "\n",
        "print(\"Average uncertainty of the model \", (points/TotalPredictions)*100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average uncertainty of the model  81.95951606424633 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f94cb7c3-1ace-4257-d9ce-6046be970fbc",
        "id": "YJUJ1t9CTAn7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "points=0\n",
        "TotalPredictions = 0\n",
        "\n",
        "for i in range(0, len(TestLines)):\n",
        "    # Verify if the correct word is in the top-50 words predicted\n",
        "    candidates = generatecandidates(X_line[i])\n",
        "    if Y_line[i] == '<Unkown>':\n",
        "        continue\n",
        "    else:\n",
        "        if Y_line[i] in candidates.keys():\n",
        "            points+=1\n",
        "            TotalPredictions +=1\n",
        "        else:\n",
        "            TotalPredictions +=1\n",
        "    \n",
        "    \n",
        "print(\"Percentage of times next word was on top 50 predictions: \",(points/TotalPredictions)*100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of times next word was on top 50 predictions:  59.197104310628504 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jb0MLIek91C2"
      },
      "source": [
        "## Evaluate New Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e7fb6268-fc7f-49f1-a61c-7f6d8c877ae3",
        "id": "dXj-OFhy91C5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        }
      },
      "source": [
        "model = tf.keras.models.load_model('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/trained_models/trained_models_model_01May2020.h5')\n",
        "\n",
        "# Read Vocabulary of data\n",
        "vocab = []\n",
        "\n",
        "with open('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/trained_models/trained_models_vocab_01May2020.data', 'rb') as filehandle:\n",
        "    # read the data as binary data stream\n",
        "    vocab = pickle.load(filehandle)\n",
        "    \n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-5237d770d5e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/SharpestMinds/3. Generate Language Model/trained_models/trained_models_model_01May2020.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Read Vocabulary of data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    182\u001b[0m     if (h5py is not None and (\n\u001b[1;32m    183\u001b[0m         isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[0;32m--> 184\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_from_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     model = model_config_lib.model_from_config(model_config,\n\u001b[0;32m--> 178\u001b[0;31m                                                custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m# set weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/model_config.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                     '`Sequential.from_config(config)`?')\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       printable_module_name='layer')\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    371\u001b[0m             custom_objects=dict(\n\u001b[1;32m    372\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_GLOBAL_CUSTOM_OBJECTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    374\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer_config\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_configs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m       layer = layer_module.deserialize(layer_config,\n\u001b[0;32m--> 398\u001b[0;31m                                        custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m    399\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     if (not model.inputs and build_input_shape and\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       printable_module_name='layer')\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    371\u001b[0m             custom_objects=dict(\n\u001b[1;32m    372\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_GLOBAL_CUSTOM_OBJECTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    374\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/wrappers.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     layer = super(Bidirectional, cls).from_config(config,\n\u001b[0;32m--> 743\u001b[0;31m                                                   custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m    744\u001b[0m     \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_constants\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_constants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/wrappers.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     layer = deserialize_layer(\n\u001b[0;32m---> 85\u001b[0;31m         config.pop('layer'), custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       printable_module_name='layer')\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midentifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     (cls, cls_config) = class_and_config_for_serialized_keras_object(\n\u001b[0;32m--> 362\u001b[0;31m         config, module_objects, custom_objects, printable_module_name)\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'from_config'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[0;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    319\u001b[0m   \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_registered_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprintable_module_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m   \u001b[0mcls_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unknown layer: UnifiedLSTM"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7f8d0e28-1067-495f-fa43-c75488a6e27a",
        "id": "bj2drMRm91DH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Read embedded spaces of vocab data\n",
        "\n",
        "embedded = np.load('/content/drive/My Drive/SharpestMinds/3. Generate Language Model/LSTM_model/DocsToLoad/embedded.npy')\n",
        "\n",
        "print(embedded.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5567, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mrm5k66E91DP",
        "colab": {}
      },
      "source": [
        "def generateInputArray(inputs):\n",
        "  embDim = embedded.shape[1]\n",
        "  x_sample =[]\n",
        "  for x in inputs:\n",
        "    if x in vocab:\n",
        "      x_sample.append(embedded[list(vocab).index(x)])\n",
        "    else:\n",
        "      x_sample.append(np.zeros(embDim))\n",
        "\n",
        "  x_sample = np.array(x_sample)\n",
        "  x_sample = np.expand_dims(x_sample, axis=0)\n",
        "  return(x_sample)\n",
        "  \n",
        "def generatecandidates(text):\n",
        "  textToken = tf.keras.preprocessing.text.text_to_word_sequence(str(text),  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r0123456789'+\"'\")   \n",
        "  \n",
        "  x_sample = generateInputArray(textToken[-5:])\n",
        "  \n",
        "  y_sample = model.predict(x_sample)\n",
        "  \n",
        "  # Get top 50 candidates\n",
        "  ind = np.argpartition(y_sample[0,:], -50)[-50:]\n",
        "\n",
        "  candidates=dict()\n",
        "  for i in ind:\n",
        "    if vocab[i] != \"<Unkown>\":\n",
        "      candidates[vocab[i]] = y_sample[0,i]\n",
        "\n",
        "  return(candidates)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d_0fXtJz91DS",
        "colab": {}
      },
      "source": [
        "X_line, Y_line = generate_text_sequences(TestLines, pastWords, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "75f98f05-efe2-4187-8c98-d4ca18b6f9e7",
        "id": "UX8KA5KQ91DU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "points=0\n",
        "TotalPredictions = 0\n",
        "for i in range(0, len(TestLines)):\n",
        "    # Verify if the correct word is in the top-50 words predicted\n",
        "    candidates = generatecandidates(X_line[i])\n",
        "    if Y_line[i] == '<Unkown>':\n",
        "        continue\n",
        "    else:\n",
        "        if Y_line[i] in candidates.keys():\n",
        "            word_prob = list(candidates.keys()).index(Y_line[i])\n",
        "            p = np.array(list(candidates.values()))\n",
        "            points += 1 - p[word_prob] \n",
        "            TotalPredictions +=1\n",
        "        else:\n",
        "            points+=1\n",
        "            TotalPredictions +=1\n",
        "\n",
        "print(\"Average uncertainty of the model \", (points/TotalPredictions)*100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average uncertainty of the model  81.95951606424633 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f94cb7c3-1ace-4257-d9ce-6046be970fbc",
        "id": "LMWigmXK91DW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "points=0\n",
        "TotalPredictions = 0\n",
        "\n",
        "for i in range(0, len(TestLines)):\n",
        "    # Verify if the correct word is in the top-50 words predicted\n",
        "    candidates = generatecandidates(X_line[i])\n",
        "    if Y_line[i] == '<Unkown>':\n",
        "        continue\n",
        "    else:\n",
        "        if Y_line[i] in candidates.keys():\n",
        "            points+=1\n",
        "            TotalPredictions +=1\n",
        "        else:\n",
        "            TotalPredictions +=1\n",
        "    \n",
        "    \n",
        "print(\"Percentage of times next word was on top 50 predictions: \",(points/TotalPredictions)*100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of times next word was on top 50 predictions:  59.197104310628504 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-zrvqcg091Da",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}